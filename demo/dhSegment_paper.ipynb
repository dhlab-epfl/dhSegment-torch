{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#dhSegment-demonstration\" data-toc-modified-id=\"dhSegment-demonstration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>dhSegment demonstration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Preparing-the-data\" data-toc-modified-id=\"Preparing-the-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Preparing the data</a></span></li><li><span><a href=\"#Training-the-model\" data-toc-modified-id=\"Training-the-model-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Training the model</a></span></li><li><span><a href=\"#Inference-and-new-annotations\" data-toc-modified-id=\"Inference-and-new-annotations-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Inference and new annotations</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Klrqc_pe_zna"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dhlab-epfl/dhSegment-torch/blob/master/demo/dhSegment_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# dhSegment demonstration\n",
    "\n",
    "This notebook will show a demonstration which will:\n",
    "1. Read a VIA annotation file and produce the necessary data for training dhSegment\n",
    "2. Train a dhSegment model\n",
    "3. Use the dhSegment model to predict new annotatations and save them in a VIA format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpSV--EVBEDg"
   },
   "source": [
    "## Setup\n",
    "\n",
    "The first 3 cells install dhSegment on the colab notebook, make the necessary imports and load the tensorboard extension to see the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scELzjYwfV3e"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/dhlab-epfl/dhSegment-torch.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icghdDp_cSyO"
   },
   "outputs": [],
   "source": [
    "from dh_segment_torch.config import Params\n",
    "\n",
    "from dh_segment_torch.data import DataSplitter\n",
    "from dh_segment_torch.data.annotation import AnnotationWriter\n",
    "\n",
    "from dh_segment_torch.training import Trainer\n",
    "\n",
    "from dh_segment_torch.inference import PredictProcess\n",
    "from dh_segment_torch.post_processing import PostProcessingPipeline\n",
    "\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9nf7gebuT0G"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ebiu1veqE-u2"
   },
   "source": [
    "In order to be able to train dhSegment effectively, a GPU is necessary, the following cell will tell you if you enabled the GPU runtime of the Google Colab and how many memory you have available.\n",
    "\n",
    "If the cell gives an error, check that you have a 'GPU' runtime in the menu > Runtime > Change Runtime Type > Hardware Accelerator. Then rerun all the above cells again.\n",
    "\n",
    "If the cells runs sucessfully, it will give you the amount of GPU memory available, please remember it as it will be used to tweak the training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHPvk6e3E8eI"
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() >= 1\n",
    "\n",
    "print(\"The GPU has %.2f GB of memory.\"%(torch.cuda.get_device_properties(0).total_memory//1024**2/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmfW_TACBsVD"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "The following cell contains the parameters for loading the data and creating the necessary data for training.\n",
    "\n",
    "Important parameters are\n",
    "- `file_path` which is the path to the annotation file.\n",
    "- `attrib_name` which is the name of the attribute to consider in the via file\n",
    "- `images_dir` which is the directory containing the images (if not using iiif)\n",
    "\n",
    "Any VIA annotations can be used with this demo. We provide annotations and images for a venetian document and annotations of columns and rows. It is available here: https://drive.switch.ch/index.php/s/t63roYZBEUZIl1U.\n",
    "\n",
    "The file should be downloaded, unzipped and either uploaded on your personal google drive acccount or uploaded directly to this notebook.\n",
    "\n",
    "To see if the file are there, you can click on the folder icon in the left panel and mount your own GDrive account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1N8jExqdVCF"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'data_path' : '/content/data', # Path to write the data\n",
    "    'data_splitter': {'train_ratio': 0.8, 'val_ratio': 0.2, 'test_ratio': 0.0}, # splitting ratio of the data\n",
    "    'annotation_reader': {\n",
    "         'type': 'via2_project', # File format of the data\n",
    "         'attrib_name': 'lines', # Name of the via attribute where the labels are defined\n",
    "         'file_path': '/content/drive/My Drive/sample_catastici/via_catastici_annotated.json', # Path to the annotation file\n",
    "         'images_dir': '/content/drive/My Drive/sample_catastici', # Path to the images directory (not necessary if using IIIF)\n",
    "         'line_thickness': 4, # Thickness of the lines to draw\n",
    "    },\n",
    "    'color_labels': {\n",
    "        'type': 'colors', # Definition of colors for each labels\n",
    "        'colors': ['#93be59', '#be5993'], # Colors\n",
    "        'labels': ['row', 'column'] # Corresponding labels\n",
    "    },\n",
    "    'copy_images': True, # Whether to copy the images\n",
    "    'overwrite': True, # Whether to overwrite the images\n",
    "    'progress': True, # Whether to show progress\n",
    "    'resizer': {'height': 1100} # Size to which the images should be resized while importing them (useful, since we resize them anyway in the processing pipeline)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8qHiDqEDUy9"
   },
   "source": [
    "The following cell prepares the data according to the parameters defined above. It is not necessary to understand its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKQxfjRKhI7j"
   },
   "outputs": [],
   "source": [
    "num_processes = params.pop(\"num_processes\", 4)\n",
    "\n",
    "data_path = params.pop(\"data_path\")\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "relative_path = params.pop(\"relative_path\", True)\n",
    "\n",
    "params.setdefault(\"labels_dir\", os.path.join(data_path, \"labels\"))\n",
    "labels_dir = params.get(\"labels_dir\")\n",
    "\n",
    "params.setdefault(\"images_dir\", os.path.join(data_path, \"images\"))\n",
    "images_dir = params.get(\"images_dir\")\n",
    "\n",
    "params.setdefault(\n",
    "    \"color_labels_file_path\", os.path.join(data_path, \"color_labels.json\")\n",
    ")\n",
    "params.setdefault(\"csv_path\", os.path.join(data_path, \"data.csv\"))\n",
    "\n",
    "data_splitter_params = params.pop(\"data_splitter\", None)\n",
    "train_csv_path = params.pop(\"train_csv\", os.path.join(data_path, \"train.csv\"))\n",
    "val_csv_path = params.pop(\"val_csv\", os.path.join(data_path, \"val.csv\"))\n",
    "test_csv_path = params.pop(\"test_csv\", os.path.join(data_path, \"test.csv\"))\n",
    "\n",
    "params.setdefault(\"type\", \"image\")\n",
    "image_writer = AnnotationWriter.from_params(params)\n",
    "data = image_writer.write(num_processes)\n",
    "\n",
    "if relative_path:\n",
    "    data['image'] = data['image'].apply(lambda path: os.path.join(\"images\", os.path.basename(path)))\n",
    "    data['label'] = data['label'].apply(lambda path: os.path.join(\"labels\", os.path.basename(path)))\n",
    "\n",
    "if data_splitter_params:\n",
    "    data_splitter = DataSplitter.from_params(data_splitter_params)\n",
    "    data_splitter.split_data(data, train_csv_path, val_csv_path, test_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6016tYIzD4dl"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "The following cell contains the configuration for the model and its training.\n",
    "\n",
    "The defaults should be fine for most use cases.\n",
    "\n",
    "The only parameter that may require tweaking is the batch size that needs to be set according to the amount of memory the GPU has. If the GPU size was above 14GB, a batch size of 4 is fine, otherwise a batch size of 2 should be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7FZhLGQoTlJ"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"color_labels\": {\"label_json_file\": '/content/data/color_labels.json'}, # Color labels produced before\n",
    "        \"train_dataset\": {\n",
    "            \"type\": \"image_csv\", # Image csv dataset\n",
    "            \"csv_filename\": \"/content/data/train.csv\",\n",
    "            \"base_dir\": \"/content/data\",\n",
    "            \"repeat_dataset\": 4, # Repeat 4 times the data since we have little\n",
    "            \"compose\": {\"transforms\": [{\"type\": \"fixed_size_resize\", \"output_size\": 1e6}]} # Resize to a fixed size, could add other transformations.\n",
    "        },\n",
    "        \"val_dataset\": {\n",
    "            \"type\": \"image_csv\", # Validation dataset\n",
    "            \"csv_filename\": \"/content/data/val.csv\",\n",
    "            \"base_dir\": \"/content/data\",\n",
    "            \"compose\": {\"transforms\": [{\"type\": \"fixed_size_resize\", \"output_size\": 1e6}]}\n",
    "        },\n",
    "        \"model\": { # Model definition, original dhSegment\n",
    "            \"encoder\": \"resnet50\", \n",
    "            \"decoder\": {\n",
    "                \"decoder_channels\": [512, 256, 128, 64, 32],\n",
    "                \"max_channels\": 512\n",
    "            }\n",
    "        },\n",
    "        \"metrics\": [['miou', 'iou'], ['iou', {\"type\": 'iou', \"average\": None}], 'precision'], # Metrics to compute\n",
    "        \"optimizer\": {\"lr\": 1e-4}, # Learning rate\n",
    "        \"lr_scheduler\": {\"type\": \"exponential\", \"gamma\": 0.9995}, # Exponential decreasing learning rate\n",
    "        \"val_metric\": \"+miou\", # Metric to observe to consider a model better than another, the + indicates that we want to maximize\n",
    "        \"early_stopping\": {\"patience\": 4}, # Number of validation steps without increase to tolerate, stops if reached\n",
    "        \"model_out_dir\": \"./model_test\", # Path to model output\n",
    "        \"num_epochs\": 100, # Number of epochs for training\n",
    "        \"evaluate_every_epoch\": 5, # Number of epochs between each validation of the model\n",
    "        \"batch_size\": 4, # Batch size (to be changed if the allocated GPU has little memory)\n",
    "        \"num_data_workers\": 0,\n",
    "        \"track_train_metrics\": False,\n",
    "        \"loggers\": [\n",
    "           {\"type\": 'tensorboard', \"log_dir\": \"./model_cadaster_test/log\", \"log_every\": 5, \"log_images_every\": 10}, # Tensorboard logging\n",
    "           ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4W-yLsfHfjB"
   },
   "source": [
    "The following cell will start and launch a Tensorboard instance. If it fails, relaunch the cell and wait until you see the Tensorboard orange bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UTZOr7nvufW"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHKacE1IHsbM"
   },
   "source": [
    "The following cell generates the trainer from the parameters defined above and trains the model.\n",
    "\n",
    "The trainig process can then be observed in the above tensorboard window.\n",
    "\n",
    "The can be interrupted if a good enough result has been obtained has checkpoints are automatically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JL95KcWYpzGH"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer.from_params(params)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlLpCDSnIFoD"
   },
   "source": [
    "## Inference and new annotations\n",
    "\n",
    "This part will be be documented at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JShF8uO8ttWm"
   },
   "outputs": [],
   "source": [
    "\n",
    "post_process_params = {\n",
    "    'type': 'dag',\n",
    "    'operations' : {\n",
    "    \"h_lines\": {\n",
    "        \"inputs\": \"h_probs\",\n",
    "        \"ops\": [\n",
    "            {\"type\": \"filter_gaussian\", \"sigma\": 1.2},\n",
    "            {\n",
    "                \"type\": \"threshold_hysteresis\",\n",
    "                \"low_threshold\": 0.1,\n",
    "                \"high_threshold\": 0.4,\n",
    "            },\n",
    "            {\"type\": \"horizontal_lines_page\", \"angle_variance\": 5, \"vote_threshold\": 100},\n",
    "            {\"type\": \"lines_filter\", \"dist_thresh\": 40},\n",
    "            \"to_line\",\n",
    "            {'type': \"assign_label\", \"label\": \"row\"}\n",
    "        ],\n",
    "    },\n",
    "    \"v_lines\": {\n",
    "        \"inputs\": \"v_probs\",\n",
    "        \"ops\": [\n",
    "            {\"type\": \"filter_gaussian\", \"sigma\": 1.2},\n",
    "            {\n",
    "                \"type\": \"threshold_hysteresis\",\n",
    "                \"low_threshold\": 0.1,\n",
    "                \"high_threshold\": 0.4,\n",
    "            },\n",
    "            {\"type\": \"vertical_lines_page\", \"angle_variance\": 5, \"vote_threshold\": 100},\n",
    "            {\"type\": \"lines_filter\", \"dist_thresh\": 40},\n",
    "            \"to_line\",\n",
    "            {'type': \"assign_label\", \"label\": \"column\"}\n",
    "        ],\n",
    "    },\n",
    "    \"mask_size\": {\n",
    "        \"inputs\": \"h_probs\",\n",
    "        \"ops\": \"probas_to_image_size\"\n",
    "    },\n",
    "    'labels_annotations': {\n",
    "        'inputs': ['h_lines', 'v_lines'],\n",
    "        \"ops\": [\"concat_lists\", 'to_labels_annotations']\n",
    "    },\n",
    "    'labels_annotations_normalized': {\n",
    "        'inputs': ['mask_size', 'labels_annotations'],\n",
    "        'ops': \"normalize_labels_annotations\"\n",
    "    },\n",
    "    'annotation': {\n",
    "        'inputs': ['path', 'labels_annotations_normalized'],\n",
    "        'ops': 'to_annotation'\n",
    "    }\n",
    "}\n",
    "\n",
    "}\n",
    "\n",
    "dataset_params = {\n",
    "    \"type\": \"folder\",\n",
    "    \"folder\": \"/content/drive/My Drive/sample_catastici\",\n",
    "    \"pre_processing\": {\"transforms\": [{\"type\": \"fixed_size_resize\", \"output_size\": 1e6}]}\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    \"model\": {\n",
    "            \"encoder\": \"resnet50\",\n",
    "            \"decoder\": {\"decoder_channels\": [512, 256, 128, 64, 32], \"max_channels\": 512}\n",
    "        },\n",
    "        \"num_classes\": 3,\n",
    "        \"model_state_dict\": \"./model_test/best_checkpoint\", # To be completed\n",
    "        \"device\": \"cuda:0\"\n",
    "}\n",
    "\n",
    "\n",
    "process_params = Params({\n",
    "    'data': dataset_params,\n",
    "    'model': model_params,\n",
    "    'post_process': post_process_params,\n",
    "    'batch_size': 4,\n",
    "    'num_workers': 4,\n",
    "    'index_to_name': {1: 'h_probs', 2: 'v_probs'},\n",
    "    'output_names': 'annotation',\n",
    "    'add_path': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDKN2dnlFms6"
   },
   "outputs": [],
   "source": [
    "predict_annots = PredictProcess.from_params(process_params)\n",
    "annots = predict_annots.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9H2plAAFpNO"
   },
   "outputs": [],
   "source": [
    "annotation_writer_params = Params({\n",
    "         'type': 'via2',\n",
    "         'attrib_name': 'type',\n",
    "         'json_path': './annotations.json',\n",
    "    \n",
    "        'annotation_iterator': annots\n",
    "    })\n",
    "writer = AnnotationWriter.from_params(annotation_writer_params)\n",
    "writer.write(num_workers=8)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dhSegment demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
